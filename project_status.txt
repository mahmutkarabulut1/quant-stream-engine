=== DIRECTORY STRUCTURE ===
.
./analytics-engine
./analytics-engine/prediction-engine
./data-collector
./data-collector/.mvn
./data-collector/.mvn/wrapper
./data-collector/src
./data-collector/src/main
./data-collector/src/main/java
./data-collector/src/main/java/com
./data-collector/src/main/java/com/quantstream
./data-collector/src/main/java/com/quantstream/datacollector
./data-collector/src/main/java/com/quantstream/datacollector/service
./data-collector/src/main/resources
./data-collector/src/main/resources/static
./data-collector/src/main/resources/templates
./data-collector/src/test
./data-collector/src/test/java
./data-collector/src/test/java/com
./data-collector/target
./data-collector/target/classes
./data-collector/target/classes/com
./data-collector/target/classes/com/example
./data-collector/target/classes/com/example/demo
./data-collector/target/classes/com/quantstream
./data-collector/target/classes/com/quantstream/datacollector
./data-collector/target/classes/com/quantstream/datacollector/service
./data-collector/target/generated-sources
./data-collector/target/generated-sources/annotations
./data-collector/target/generated-test-sources
./data-collector/target/generated-test-sources/test-annotations
./data-collector/target/maven-archiver
./data-collector/target/maven-status
./data-collector/target/maven-status/maven-compiler-plugin
./data-collector/target/maven-status/maven-compiler-plugin/compile
./data-collector/target/maven-status/maven-compiler-plugin/compile/default-compile
./data-collector/target/maven-status/maven-compiler-plugin/testCompile
./data-collector/target/maven-status/maven-compiler-plugin/testCompile/default-testCompile
./data-collector/target/surefire-reports
./data-collector/target/test-classes
./data-collector/target/test-classes/com
./data-collector/target/test-classes/com/example
./data-collector/target/test-classes/com/example/demo
./infra
./k8s

=== FILE CONTENTS ===

---------------------------------------------------
FILE: ./analytics-engine/dashboard.py
---------------------------------------------------
import os
import time
import json
import pandas as pd
import numpy as np
import streamlit as st
import plotly.graph_objects as go
from kafka import KafkaConsumer
from datetime import datetime
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# Page Configuration
st.set_page_config(page_title="QuantStream Hybrid AI", layout="wide")
st.title("BTC/USDT HYBRID AI-POWERED ANALYTICS")

# Professional UI Styling (Original + AI Enhancements)
st.markdown("""
    <style>
    .report-box {
        background-color: #0E1117;
        padding: 25px;
        border-radius: 12px;
        border: 1px solid #30363d;
        border-left: 6px solid #ff00ff;
        margin-top: 25px;
        color: #E6EDF3;
    }
    .report-title {
        color: #ff00ff;
        font-size: 24px;
        font-weight: 700;
        margin-bottom: 15px;
    }
    .report-item {
        margin-bottom: 12px;
        font-size: 16px;
        line-height: 1.6;
        color: #C9D1D9;
    }
    .label {
        color: #8B949E;
        font-weight: 500;
        margin-right: 5px;
    }
    .value {
        color: #FFFFFF;
        font-weight: 600;
    }
    .signal-buy { color: #00ff88; font-weight: 800; }
    .signal-sell { color: #FF4B4B; font-weight: 800; }
    .signal-neutral { color: #FFD700; font-weight: 800; }
    .risk-high { color: #FF4B4B; font-weight: 800; }
    .risk-medium { color: #FFA500; font-weight: 800; }
    .risk-low { color: #00FF88; font-weight: 800; }
    </style>
""", unsafe_allow_html=True)

# Kafka Connection - Listening to AI Engine Output
@st.cache_resource
def get_kafka_consumer():
    KAFKA_URL = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-service:9092')
    try:
        return KafkaConsumer(
            'ai-predictions',
            bootstrap_servers=[KAFKA_URL],
            auto_offset_reset='latest',
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
            consumer_timeout_ms=1000,
            group_id="ai-dashboard-v6"
        )
    except Exception as e:
        st.error(f"Kafka Connection Error: {e}")
        return None

# Visual Analysis Functions (Restored for UI Depth)
def calculate_visual_indicators(df):
    if len(df) < 20: return df
    df['SMA_20'] = df['current_price'].rolling(window=20).mean()
    df['StdDev'] = df['current_price'].rolling(window=20).std()
    df['Upper_Band'] = df['SMA_20'] + (df['StdDev'] * 2)
    df['Lower_Band'] = df['SMA_20'] - (df['StdDev'] * 2)
    
    # Estimate VWAP proxy if quantity is not streamed from predictor
    df['pseudo_quantity'] = np.random.uniform(0.1, 5.0, size=len(df)) 
    df['VWAP'] = (df['current_price'] * df['pseudo_quantity']).cumsum() / df['pseudo_quantity'].cumsum()
    return df

def run_visual_anomaly_detection(df):
    if len(df) < 50: return df
    model = IsolationForest(contamination=0.05, random_state=42)
    data_for_ai = df[['current_price']].fillna(0)
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data_for_ai)
    df['anomaly'] = model.fit_predict(scaled_data)
    return df

# Hybrid Market Report Generator
def generate_hybrid_report(df, last_data):
    if len(df) < 50: return "Insufficient data for deep analysis."
    
    last_price = last_data['current_price']
    lstm_target = last_data['lstm_predicted_price']
    xgb_signal = last_data['xgb_signal']
    std_dev = df['StdDev'].iloc[-1] if 'StdDev' in df.columns else 0
    anomalies = df[df['anomaly'] == -1].shape[0] if 'anomaly' in df.columns else 0
    
    # Restored Risk Score Calculation
    anomaly_risk = min(40, anomalies * 2)
    vol_risk = min(30, (std_dev / last_price) * 10000)
    total_risk = min(100, anomaly_risk + vol_risk + 15)
    
    if total_risk > 70: risk_status, r_class = "CRITICAL", "risk-high"
    elif total_risk > 40: risk_status, r_class = "ELEVATED", "risk-medium"
    else: risk_status, r_class = "STABLE", "risk-low"

    # XGBoost Signal CSS
    if xgb_signal == "BUY": s_class = "signal-buy"
    elif xgb_signal == "SELL": s_class = "signal-sell"
    else: s_class = "signal-neutral"

    spread = lstm_target - last_price

    return f"""
    <div class='report-box'>
        <div class='report-title'>Hybrid Intelligence & Risk Analysis</div>
        <div class='report-item'><span class='label'>Market Risk Score:</span> <span class='{r_class}'>{total_risk:.0f}/100 ({risk_status})</span></div>
        <div class='report-item'><span class='label'>XGBoost Action Signal:</span> <span class='{s_class}'>{xgb_signal}</span></div>
        <div class='report-item'><span class='label'>LSTM Next Target:</span> <span class='value'>${lstm_target:.2f} (Spread: ${spread:.2f})</span></div>
        <div class='report-item'><span class='label'>Volatility Index:</span> <span class='value'>{std_dev:.2f} (Based on StdDev)</span></div>
        <div class='report-item'><span class='label'>Anomalous Events:</span> <span class='value'>{anomalies} Patterns Detected by Isolation Forest</span></div>
        <div style='margin-top: 15px; color: #8B949E; font-size: 12px; font-style: italic;'>
            Hybrid AI Engine Refresh: {datetime.now().strftime('%H:%M:%S')}
        </div>
    </div>
    """

# Buffer Initialization
if 'buffer' not in st.session_state:
    st.session_state.buffer = []
if 'last_report_time' not in st.session_state:
    st.session_state.last_report_time = 0
if 'latest_report' not in st.session_state:
    st.session_state.latest_report = "Initializing Hybrid Risk Engine..."

# Main Dashboard Fragment
@st.fragment(run_every=1)
def analytics_dashboard():
    consumer = get_kafka_consumer()
    if not consumer: return
    
    msg_pack = consumer.poll(timeout_ms=500)
    for tp, messages in msg_pack.items():
        for msg in messages:
            data = msg.value
            ts = data['timestamp'] / 1000.0 if data['timestamp'] > 1e11 else data['timestamp']
            
            st.session_state.buffer.append({
                "time": datetime.fromtimestamp(ts),
                "current_price": data['current_price'],
                "lstm_predicted_price": data['lstm_predicted_price'],
                "xgb_signal": data['xgb_signal'],
                "rsi": data['rsi']
            })

    if len(st.session_state.buffer) > 500:
        st.session_state.buffer = st.session_state.buffer[-500:]

    if len(st.session_state.buffer) > 20:
        df = pd.DataFrame(st.session_state.buffer)
        df = calculate_visual_indicators(df)
        df = run_visual_anomaly_detection(df)
        
        last_data = df.iloc[-1]
        
        current_time = time.time()
        if current_time - st.session_state.last_report_time > 5:
            st.session_state.latest_report = generate_hybrid_report(df, last_data)
            st.session_state.last_report_time = current_time

        # Plotly Charts - Restored 4-Chart Layout
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Price, Bollinger Bands & LSTM Target")
            fig1 = go.Figure()
            fig1.add_trace(go.Scatter(x=df['time'], y=df['current_price'], name='Price', line=dict(color='#00ff88')))
            fig1.add_trace(go.Scatter(x=df['time'], y=df['Upper_Band'], name='Upper BB', line=dict(color='gray', dash='dash')))
            fig1.add_trace(go.Scatter(x=df['time'], y=df['Lower_Band'], name='Lower BB', line=dict(color='gray', dash='dash'), fill='tonexty'))
            fig1.add_trace(go.Scatter(x=df['time'], y=df['lstm_predicted_price'], name='LSTM Prediction', line=dict(color='#ff00ff', dash='dot')))
            fig1.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0))
            st.plotly_chart(fig1, use_container_width=True)
            
        with col2:
            st.subheader("AI Engine RSI Momentum")
            fig2 = go.Figure(go.Scatter(x=df['time'], y=df['rsi'], name='AI RSI', line=dict(color='#00ccff')))
            fig2.add_hline(y=70, line_dash="dot", line_color="red")
            fig2.add_hline(y=30, line_dash="dot", line_color="green")
            fig2.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0), yaxis_range=[0,100])
            st.plotly_chart(fig2, use_container_width=True)

        col3, col4 = st.columns(2)
        
        with col3:
            st.subheader("XGBoost Signals & VWAP Trend")
            fig3 = go.Figure()
            fig3.add_trace(go.Scatter(x=df['time'], y=df['current_price'], name='Price', line=dict(color='#444')))
            fig3.add_trace(go.Scatter(x=df['time'], y=df['VWAP'], name='VWAP', line=dict(color='#ffa500', width=2)))
            
            # XGBoost Signals Overlay
            buy_signals = df[df['xgb_signal'] == 'BUY']
            sell_signals = df[df['xgb_signal'] == 'SELL']
            if not buy_signals.empty:
                fig3.add_trace(go.Scatter(x=buy_signals['time'], y=buy_signals['current_price'], mode='markers', name='BUY', marker=dict(color='#00ff88', size=10, symbol='triangle-up')))
            if not sell_signals.empty:
                fig3.add_trace(go.Scatter(x=sell_signals['time'], y=sell_signals['current_price'], mode='markers', name='SELL', marker=dict(color='#ff4b4b', size=10, symbol='triangle-down')))
            
            fig3.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0))
            st.plotly_chart(fig3, use_container_width=True)
            
        with col4:
            st.subheader("Isolation Forest Anomaly Detection")
            fig4 = go.Figure()
            fig4.add_trace(go.Scatter(x=df['time'], y=df['current_price'], name='Normal', line=dict(color='#333')))
            if 'anomaly' in df.columns:
                anomalies = df[df['anomaly'] == -1]
                if not anomalies.empty:
                    fig4.add_trace(go.Scatter(x=anomalies['time'], y=anomalies['current_price'], mode='markers', name='Anomaly', marker=dict(color='red', size=8, symbol='x')))
            fig4.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0))
            st.plotly_chart(fig4, use_container_width=True)

        st.markdown(st.session_state.latest_report, unsafe_allow_html=True)
    else:
        st.info("Gathering AI prediction data. Waiting for buffer...")

analytics_dashboard()
---------------------------------------------------
FILE: ./analytics-engine/Dockerfile
---------------------------------------------------
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY analyzer.py .
COPY dashboard.py .
COPY entrypoint.sh .
RUN chmod +x entrypoint.sh
EXPOSE 8501
CMD ["/bin/bash", "entrypoint.sh"]

---------------------------------------------------
FILE: ./analytics-engine/entrypoint.sh
---------------------------------------------------
#!/bin/bash
set -e

echo "üîå Sistem Ba≈ülatƒ±lƒ±yor..."

# Kafka Bekleme
python3 -u -c "import socket, time; 
host, port = 'kafka-service', 9092; 
print(f'Waiting for Kafka at {host}:{port}...'); 
for _ in range(30):
    try:
        socket.create_connection((host, port), timeout=5); 
        print('‚úÖ Kafka Ready!'); 
        break
    except: 
        time.sleep(2)"

# Producer Ba≈ülat
echo "üöÄ Veri Motoru Ba≈ülatƒ±lƒ±yor..."
python3 -u analyzer.py &

# Dashboard Ba≈ülat
echo "üìä Dashboard Ba≈ülatƒ±lƒ±yor..."
exec streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0

---------------------------------------------------
FILE: ./analytics-engine/prediction-engine/evaluator.py
---------------------------------------------------
import os
import json
import pandas as pd
import numpy as np
from kafka import KafkaConsumer
from datetime import datetime
from collections import deque

# Kafka Configuration
KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-service:9092')
INPUT_TOPIC = 'ai-predictions'

class ModelEvaluator:
    def __init__(self):
        self.history = []
        self.last_prediction = None
        self.total_samples = 0
        self.correct_directions = 0
        self.lstm_errors = []
        
        # Buffer for calculating rolling accuracy (last 1000 trades)
        self.rolling_window = deque(maxlen=1000)

    def calculate_metrics(self):
        if self.total_samples == 0:
            return 0.0, 0.0

        # Directional Accuracy (XGBoost)
        dir_accuracy = (self.correct_directions / self.total_samples) * 100
        
        # LSTM Error (Mean Absolute Percentage Error)
        avg_lstm_error = np.mean(self.lstm_errors) if self.lstm_errors else 0.0
        
        return dir_accuracy, avg_lstm_error

    def process_message(self, message):
        current_data = message.value
        current_price = current_data['current_price']
        timestamp = current_data['timestamp']
        
        # If we have a prediction from the PREVIOUS step, validate it now
        if self.last_prediction is not None:
            prev_price = self.last_prediction['current_price']
            predicted_lstm_price = self.last_prediction['lstm_predicted_price']
            predicted_signal = self.last_prediction['xgb_signal']
            
            # 1. Evaluate XGBoost (Directional Accuracy)
            actual_movement = "HOLD"
            if current_price > prev_price:
                actual_movement = "BUY"
            elif current_price < prev_price:
                actual_movement = "SELL"
            
            is_correct_direction = (predicted_signal == actual_movement)
            
            # 2. Evaluate LSTM (Price Accuracy)
            # Calculate absolute percentage error
            error_margin = abs(current_price - predicted_lstm_price)
            error_percentage = (error_margin / current_price) * 100
            
            # Update Stats
            self.total_samples += 1
            if is_correct_direction:
                self.correct_directions += 1
            
            self.lstm_errors.append(error_percentage)
            
            # Log for CSV
            log_entry = {
                'timestamp': timestamp,
                'prev_price': prev_price,
                'actual_price': current_price,
                'lstm_predicted': predicted_lstm_price,
                'xgb_signal': predicted_signal,
                'actual_movement': actual_movement,
                'direction_correct': is_correct_direction,
                'lstm_error_pct': error_percentage
            }
            self.history.append(log_entry)
            
            # Periodic Console Report (Every 10 samples)
            if self.total_samples % 10 == 0:
                acc, mape = self.calculate_metrics()
                print(f"[{datetime.now().strftime('%H:%M:%S')}] Sample: {self.total_samples} | "
                      f"XGBoost Accuracy: {acc:.2f}% | "
                      f"LSTM Error Margin: {mape:.4f}%", flush=True)

                # Auto-save report every 50 samples
                if self.total_samples % 50 == 0:
                    self.save_report()

        # Store current prediction for NEXT iteration validation
        self.last_prediction = current_data

    def save_report(self):
        df = pd.DataFrame(self.history)
        filename = "model_performance_report.csv"
        df.to_csv(filename, index=False)
        print(f"--> Report saved to {filename}", flush=True)

    def run(self):
        print("Model Evaluator Started... Listening to Kafka topic: " + INPUT_TOPIC, flush=True)
        consumer = KafkaConsumer(
            INPUT_TOPIC,
            bootstrap_servers=[KAFKA_BOOTSTRAP],
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )

        for msg in consumer:
            self.process_message(msg)

if __name__ == "__main__":
    evaluator = ModelEvaluator()
    evaluator.run()

---------------------------------------------------
FILE: ./analytics-engine/prediction-engine/predictor.py
---------------------------------------------------
import os
import json
import numpy as np
import pandas as pd
import joblib
from kafka import KafkaConsumer, KafkaProducer
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import warnings

# Gereksiz uyarƒ±larƒ± kapatalƒ±m (Demo sƒ±rasƒ±nda terminal kirlenmesin)
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Kafka Ayarlarƒ±
KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-service:9092')
INPUT_TOPIC = 'trade-events'
OUTPUT_TOPIC = 'ai-predictions'

class HybridAIEngine:
    def __init__(self):
        self.data_buffer = []
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.min_training_size = 60  # Modellerin eƒüitilmesi i√ßin gereken minimum veri
        
        # Kafka Producer (Tahminleri yayƒ±nlamak i√ßin)
        self.producer = KafkaProducer(
            bootstrap_servers=[KAFKA_BOOTSTRAP],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )

    def calculate_technical_features(self, df):
        """XGBoost i√ßin teknik indikat√∂rler √ºretir"""
        df['SMA_10'] = df['price'].rolling(window=10).mean()
        df['RSI'] = self.calculate_rsi(df['price'])
        df['Momentum'] = df['price'].diff(4)
        df.dropna(inplace=True)
        return df

    def calculate_rsi(self, series, period=14):
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))

    def train_predict_xgboost(self, df):
        """Sinyal Sƒ±nƒ±flandƒ±rma: 0 (Sat), 1 (Bekle), 2 (Al)"""
        # Gelecek fiyat hareketine g√∂re etiketleme (Labeling)
        df['Target'] = 1 # Bekle
        df.loc[df['price'].shift(-1) > df['price'] * 1.001, 'Target'] = 2 # Al (Y√ºkseli≈ü)
        df.loc[df['price'].shift(-1) < df['price'] * 0.999, 'Target'] = 0 # Sat (D√º≈ü√º≈ü)
        
        features = ['price', 'quantity', 'SMA_10', 'RSI', 'Momentum']
        X = df[features].iloc[:-1] # Son satƒ±r hari√ß (√ß√ºnk√º target'ƒ± yok)
        y = df['Target'].iloc[:-1]
        
        if len(X) < 10: return "WAIT (Data Gathering)"

        model = XGBClassifier(n_estimators=50, max_depth=3, eval_metric='mlogloss')
        model.fit(X, y)
        
        # Son gelen veri ile tahmin yap
        last_data = df[features].iloc[-1:].values
        prediction = model.predict(last_data)[0]
        
        signals = {0: "SELL", 1: "HOLD", 2: "BUY"}
        return signals.get(prediction, "HOLD")

    def train_predict_lstm(self, df):
        """Fiyat Tahmini (Regression)"""
        data = df['price'].values.reshape(-1, 1)
        scaled_data = self.scaler.fit_transform(data)
        
        X_train, y_train = [], []
        look_back = 10  # Son 10 i≈üleme bakarak bir sonrakini tahmin et
        
        if len(scaled_data) <= look_back: return 0.0

        for i in range(look_back, len(scaled_data)):
            X_train.append(scaled_data[i-look_back:i, 0])
            y_train.append(scaled_data[i, 0])
            
        X_train, y_train = np.array(X_train), np.array(y_train)
        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
        
        # Basit LSTM Mimarisi
        model = Sequential()
        model.add(LSTM(units=50, return_sequences=False, input_shape=(X_train.shape[1], 1)))
        model.add(Dropout(0.2))
        model.add(Dense(units=1))
        model.compile(optimizer='adam', loss='mean_squared_error')
        
        # Hƒ±zlƒ± eƒüitim (Demo i√ßin epoch d√º≈ü√ºk tutuldu)
        model.fit(X_train, y_train, epochs=1, batch_size=1, verbose=0)
        
        # Tahmin
        last_sequence = scaled_data[-look_back:].reshape(1, look_back, 1)
        predicted_scaled = model.predict(last_sequence, verbose=0)
        predicted_price = self.scaler.inverse_transform(predicted_scaled)
        
        return float(predicted_price[0][0])

    def run(self):
        print("üöÄ AI Prediction Engine Ba≈ülatƒ±ldƒ±... Kafka bekleniyor.", flush=True)
        consumer = KafkaConsumer(
            INPUT_TOPIC,
            bootstrap_servers=[KAFKA_BOOTSTRAP],
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )

        for message in consumer:
            trade = message.value
            self.data_buffer.append(trade)
            
            # Bellek y√∂netimi (Son 500 veriyi tut)
            if len(self.data_buffer) > 500:
                self.data_buffer.pop(0)

            # Yeterli veri varsa analizi ba≈ülat
            if len(self.data_buffer) > self.min_training_size:
                df = pd.DataFrame(self.data_buffer)
                df['price'] = pd.to_numeric(df['p'])
                df['quantity'] = pd.to_numeric(df['q'])
                
                # √ñzellik M√ºhendisliƒüi
                df = self.calculate_technical_features(df)
                
                # 1. Model: XGBoost ile Sinyal
                xgb_signal = self.train_predict_xgboost(df.copy())
                
                # 2. Model: LSTM ile Fiyat Tahmini
                lstm_price = self.train_predict_lstm(df.copy())
                
                # Sonu√ßlarƒ± Kafka'ya g√∂nder
                result = {
                    'timestamp': trade['t'],
                    'current_price': float(trade['p']),
                    'lstm_predicted_price': lstm_price,
                    'xgb_signal': xgb_signal,
                    'rsi': float(df['RSI'].iloc[-1]) if not pd.isna(df['RSI'].iloc[-1]) else 0
                }
                
                self.producer.send(OUTPUT_TOPIC, result)
                print(f"AI Output: LSTM Price={lstm_price:.2f} | XGB Signal={xgb_signal}", flush=True)

if __name__ == "__main__":
    engine = HybridAIEngine()
    engine.run()

---------------------------------------------------
FILE: ./analytics-engine/streamer.py
---------------------------------------------------
import os
import json
import asyncio
import aiohttp
from kafka import KafkaProducer
import sys

KAFKA_URL = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-service:9092')
TOPIC_NAME = 'trade-events'

def get_producer():
    try:
        return KafkaProducer(
            bootstrap_servers=[KAFKA_URL],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    except Exception as e:
        print(f"‚ùå Kafka Baƒülantƒ± Hatasƒ±: {e}", flush=True)
        return None

async def binance_trade_stream():
    uri = "wss://stream.binance.com:9443/ws/btcusdt@trade"
    producer = get_producer()
    
    if not producer:
        print("‚ùå Producer ba≈ülatƒ±lamadƒ±!", flush=True)
        return

    print(f"üöÄ Binance'e baƒülanƒ±lƒ±yor: {uri}", flush=True)
    
    async with aiohttp.ClientSession() as session:
        async with session.ws_connect(uri) as ws:
            async for msg in ws:
                if msg.type == aiohttp.WSMsgType.TEXT:
                    try:
                        trade_data = json.loads(msg.data)
                        payload = {'t': trade_data['T'], 'p': trade_data['p'], 'q': trade_data['q']}
                        producer.send(TOPIC_NAME, payload)
                        print(f"‚úÖ Veri: {payload['p']}", flush=True)
                    except Exception as e:
                        print(f"‚ùå Veri Hatasƒ±: {e}", flush=True)
                elif msg.type == aiohttp.WSMsgType.ERROR:
                    print("‚ùå WebSocket Hatasƒ±!", flush=True)
                    break

if __name__ == "__main__":
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    while True:
        try:
            loop.run_until_complete(binance_trade_stream())
        except Exception as e:
            print(f"‚ö†Ô∏è Baƒülantƒ± koptu: {e}", flush=True)
            asyncio.sleep(5)

---------------------------------------------------
FILE: ./data-collector/Dockerfile
---------------------------------------------------
# Stage 1: Build the application
FROM maven:3.9.6-eclipse-temurin-21 AS build
WORKDIR /app
COPY pom.xml .
COPY src ./src
RUN mvn clean package -DskipTests

# Stage 2: Run the application
FROM eclipse-temurin:21-jre
WORKDIR /app
COPY --from=build /app/target/*.jar app.jar
ENTRYPOINT ["java", "-jar", "app.jar"]

---------------------------------------------------
FILE: ./docker-compose.yml
---------------------------------------------------
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - quant-network

  kafka-service:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-service:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - quant-network

  streamer:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["python", "streamer.py"]
    depends_on:
      - kafka-service
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-service:9092
    networks:
      - quant-network

  predictor:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["python", "prediction-engine/predictor.py"]
    depends_on:
      - kafka-service
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-service:9092
    networks:
      - quant-network

  evaluator:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["python", "prediction-engine/evaluator.py"]
    depends_on:
      - kafka-service
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-service:9092
    volumes:
      - ./prediction-engine:/app/prediction-engine
    networks:
      - quant-network

  dashboard:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["streamlit", "run", "dashboard.py", "--server.port=8501", "--server.address=0.0.0.0"]
    ports:
      - "8501:8501"
    depends_on:
      - kafka-service
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka-service:9092
    networks:
      - quant-network

networks:
  quant-network:
    driver: bridge

---------------------------------------------------
FILE: ./final_fix.sh
---------------------------------------------------
#!/bin/bash
set -e

echo "üõë Sistem tamamen durduruluyor..."
kubectl delete deployment dashboard-engine --ignore-not-found
kubectl delete service dashboard-service --ignore-not-found
pkill -f "kubectl port-forward" || true

echo "üßπ Minikube Docker ortamƒ±na baƒülanƒ±lƒ±yor..."
eval $(minikube docker-env)

echo "üìù DOSYALAR SIFIRDAN YAZILIYOR (GARANTƒ∞Lƒ∞ Y√ñNTEM)..."

# 1. REQUIREMENTS.TXT (Eksik olan buydu!)
cat << 'EOF_REQ' > analytics-engine/requirements.txt
kafka-python==2.0.2
pandas==2.2.0
streamlit==1.37.0
plotly==5.18.0
scikit-learn==1.4.0
aiohttp==3.9.3
EOF_REQ

# 2. DOCKERFILE (Sƒ±ralama √∂nemli)
cat << 'EOF_DOCKER' > analytics-engine/Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY analyzer.py .
COPY dashboard.py .
COPY entrypoint.sh .
RUN chmod +x entrypoint.sh
EXPOSE 8501
CMD ["/bin/bash", "entrypoint.sh"]
EOF_DOCKER

# 3. ANALYZER.PY (Producer - Loglarƒ± a√ßƒ±k)
cat << 'EOF_PY' > analytics-engine/analyzer.py
import os
import json
import asyncio
import aiohttp
from kafka import KafkaProducer
import sys

KAFKA_URL = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-service:9092')
TOPIC_NAME = 'trade-events'

def get_producer():
    try:
        return KafkaProducer(
            bootstrap_servers=[KAFKA_URL],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    except Exception as e:
        print(f"‚ùå Kafka Baƒülantƒ± Hatasƒ±: {e}", flush=True)
        return None

async def binance_trade_stream():
    uri = "wss://stream.binance.com:9443/ws/btcusdt@trade"
    producer = get_producer()
    
    if not producer:
        print("‚ùå Producer ba≈ülatƒ±lamadƒ±!", flush=True)
        return

    print(f"üöÄ Binance'e baƒülanƒ±lƒ±yor: {uri}", flush=True)
    
    async with aiohttp.ClientSession() as session:
        async with session.ws_connect(uri) as ws:
            async for msg in ws:
                if msg.type == aiohttp.WSMsgType.TEXT:
                    try:
                        trade_data = json.loads(msg.data)
                        payload = {'t': trade_data['T'], 'p': trade_data['p'], 'q': trade_data['q']}
                        producer.send(TOPIC_NAME, payload)
                        print(f"‚úÖ Veri: {payload['p']}", flush=True)
                    except Exception as e:
                        print(f"‚ùå Veri Hatasƒ±: {e}", flush=True)
                elif msg.type == aiohttp.WSMsgType.ERROR:
                    print("‚ùå WebSocket Hatasƒ±!", flush=True)
                    break

if __name__ == "__main__":
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    while True:
        try:
            loop.run_until_complete(binance_trade_stream())
        except Exception as e:
            print(f"‚ö†Ô∏è Baƒülantƒ± koptu: {e}", flush=True)
            asyncio.sleep(5)
EOF_PY

# 4. ENTRYPOINT.SH
cat << 'EOF_SH' > analytics-engine/entrypoint.sh
#!/bin/bash
set -e

echo "üîå Sistem Ba≈ülatƒ±lƒ±yor..."

# Kafka Bekleme
python3 -u -c "import socket, time; 
host, port = 'kafka-service', 9092; 
print(f'Waiting for Kafka at {host}:{port}...'); 
for _ in range(30):
    try:
        socket.create_connection((host, port), timeout=5); 
        print('‚úÖ Kafka Ready!'); 
        break
    except: 
        time.sleep(2)"

# Producer Ba≈ülat
echo "üöÄ Veri Motoru Ba≈ülatƒ±lƒ±yor..."
python3 -u analyzer.py &

# Dashboard Ba≈ülat
echo "üìä Dashboard Ba≈ülatƒ±lƒ±yor..."
exec streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0
EOF_SH
chmod +x analytics-engine/entrypoint.sh

# 5. KUBERNETES YAML
cat << 'EOF_K8S' > k8s/apps.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dashboard-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dashboard-engine
  template:
    metadata:
      labels:
        app: dashboard-engine
    spec:
      containers:
      - name: dashboard-engine
        image: mahmut/analytics-engine:v1
        imagePullPolicy: Never
        ports:
        - containerPort: 8501
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service:9092"
---
apiVersion: v1
kind: Service
metadata:
  name: dashboard-service
spec:
  selector:
    app: dashboard-engine
  ports:
    - protocol: TCP
      port: 8501
      targetPort: 8501
  type: ClusterIP
EOF_K8S

echo "üèóÔ∏è ƒ∞maj YENƒ∞DEN in≈üa ediliyor (NO-CACHE)..."
docker build --no-cache -t mahmut/analytics-engine:v1 ./analytics-engine

echo "üöÄ Kubernetes'e Deploy ediliyor..."
kubectl apply -f k8s/apps.yaml

echo "‚è≥ Pod bekleniyor..."
kubectl wait --for=condition=ready pod -l app=dashboard-engine --timeout=120s

echo "‚úÖ TAMAMLANDI! Loglar a√ßƒ±lƒ±yor..."

---------------------------------------------------
FILE: ./fix_system.sh
---------------------------------------------------
#!/bin/bash
set -e

echo "üõë Mevcut sistem durduruluyor..."
kubectl delete deployment dashboard-engine --ignore-not-found
kubectl delete service dashboard-service --ignore-not-found
pkill -f "kubectl port-forward" || true

echo "üßπ Minikube Docker ortamƒ±na baƒülanƒ±lƒ±yor..."
eval $(minikube docker-env)

echo "üóëÔ∏è Eski bozuk imajlar temizleniyor..."
docker rmi -f mahmut/analytics-engine:v1 || true

echo "üìù Kodlar g√ºncelleniyor (Garantili Loglama Modu)..."

# 1. ANALYZER.PY (Flush=True ile anlƒ±k loglama)
cat << 'EOF_PY' > analytics-engine/analyzer.py
import os
import json
import asyncio
import aiohttp
from kafka import KafkaProducer
import sys

KAFKA_URL = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-service:9092')
TOPIC_NAME = 'trade-events'

def get_producer():
    try:
        return KafkaProducer(
            bootstrap_servers=[KAFKA_URL],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    except Exception as e:
        print(f"‚ùå Kafka Baƒülantƒ± Hatasƒ±: {e}", flush=True)
        return None

async def binance_trade_stream():
    uri = "wss://stream.binance.com:9443/ws/btcusdt@trade"
    producer = get_producer()
    
    if not producer:
        print("‚ùå Producer ba≈ülatƒ±lamadƒ±!", flush=True)
        return

    print(f"üöÄ Binance'e baƒülanƒ±lƒ±yor: {uri}", flush=True)
    
    async with aiohttp.ClientSession() as session:
        async with session.ws_connect(uri) as ws:
            async for msg in ws:
                if msg.type == aiohttp.WSMsgType.TEXT:
                    try:
                        trade_data = json.loads(msg.data)
                        payload = {'t': trade_data['T'], 'p': trade_data['p'], 'q': trade_data['q']}
                        producer.send(TOPIC_NAME, payload)
                        # LOGLARI BURADA ZORLA BASIYORUZ (FLUSH=TRUE)
                        print(f"‚úÖ Veri G√∂nderildi: Fiyat={payload['p']}", flush=True)
                    except Exception as e:
                        print(f"‚ùå Veri Hatasƒ±: {e}", flush=True)
                elif msg.type == aiohttp.WSMsgType.ERROR:
                    print("‚ùå WebSocket Hatasƒ±!", flush=True)
                    break

if __name__ == "__main__":
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    while True:
        try:
            loop.run_until_complete(binance_trade_stream())
        except Exception as e:
            print(f"‚ö†Ô∏è Baƒülantƒ± koptu, 5sn sonra tekrar deneniyor: {e}", flush=True)
            asyncio.sleep(5)
EOF_PY

# 2. ENTRYPOINT.SH (Unbuffered Python -u)
cat << 'EOF_SH' > analytics-engine/entrypoint.sh
#!/bin/bash
set -e

echo "üîå Sistem Ba≈ülatƒ±lƒ±yor (vFinal)..."

# Kafka Bekleme
python3 -u -c "import socket, time; 
host, port = 'kafka-service', 9092; 
print(f'Waiting for Kafka at {host}:{port}...'); 
for _ in range(30):
    try:
        socket.create_connection((host, port), timeout=5); 
        print('‚úÖ Kafka Ready!'); 
        break
    except: 
        time.sleep(2)"

# Producer Ba≈ülat (-u parametresi √ßok √∂nemli)
echo "üöÄ Veri Motoru (Producer) Ba≈ülatƒ±lƒ±yor..."
python3 -u analyzer.py &

# Dashboard Ba≈ülat
echo "üìä Dashboard Ba≈ülatƒ±lƒ±yor..."
exec streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0
EOF_SH
chmod +x analytics-engine/entrypoint.sh

# 3. KUBERNETES YAML (ImagePullPolicy: Never)
cat << 'EOF_K8S' > k8s/apps.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dashboard-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dashboard-engine
  template:
    metadata:
      labels:
        app: dashboard-engine
    spec:
      containers:
      - name: dashboard-engine
        image: mahmut/analytics-engine:v1
        imagePullPolicy: Never
        ports:
        - containerPort: 8501
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service:9092"
---
apiVersion: v1
kind: Service
metadata:
  name: dashboard-service
spec:
  selector:
    app: dashboard-engine
  ports:
    - protocol: TCP
      port: 8501
      targetPort: 8501
  type: ClusterIP
EOF_K8S

echo "üèóÔ∏è ƒ∞maj yeniden in≈üa ediliyor (NO-CACHE)..."
# Context zaten ayarlƒ± ama garanti olsun
docker build --no-cache -t mahmut/analytics-engine:v1 ./analytics-engine

echo "üöÄ Kubernetes'e Deploy ediliyor..."
kubectl apply -f k8s/apps.yaml

echo "‚è≥ Pod'un hazƒ±r olmasƒ± bekleniyor (Max 60sn)..."
kubectl wait --for=condition=ready pod -l app=dashboard-engine --timeout=60s

echo "‚úÖ Sƒ∞STEM HAZIR! Loglar kontrol ediliyor..."

---------------------------------------------------
FILE: ./infra/docker-compose.yaml
---------------------------------------------------
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

---------------------------------------------------
FILE: ./k8s/apps.yaml
---------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dashboard-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dashboard-engine
  template:
    metadata:
      labels:
        app: dashboard-engine
    spec:
      containers:
      - name: dashboard-engine
        image: mahmut/analytics-engine:v1
        imagePullPolicy: Never
        command: ["streamlit", "run", "dashboard.py", "--server.port=8501", "--server.address=0.0.0.0"]
        ports:
        - containerPort: 8501
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service:9092"
---
apiVersion: v1
kind: Service
metadata:
  name: dashboard-service
spec:
  selector:
    app: dashboard-engine
  ports:
    - protocol: TCP
      port: 8501
      targetPort: 8501
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: streamer-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      app: streamer-engine
  template:
    metadata:
      labels:
        app: streamer-engine
    spec:
      containers:
      - name: streamer-engine
        image: mahmut/analytics-engine:v1
        imagePullPolicy: Never
        command: ["python", "streamer.py"]
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service:9092"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: predictor-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      app: predictor-engine
  template:
    metadata:
      labels:
        app: predictor-engine
    spec:
      containers:
      - name: predictor-engine
        image: mahmut/analytics-engine:v1
        imagePullPolicy: Never
        command: ["python", "prediction-engine/predictor.py"]
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service:9092"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: evaluator-engine
spec:
  replicas: 1
  selector:
    matchLabels:
      app: evaluator-engine
  template:
    metadata:
      labels:
        app: evaluator-engine
    spec:
      containers:
      - name: evaluator-engine
        image: mahmut/analytics-engine:v1
        imagePullPolicy: Never
        command: ["python", "prediction-engine/evaluator.py"]
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "kafka-service:9092"
---------------------------------------------------
FILE: ./k8s/kafka.yaml
---------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: kafka-service
spec:
  ports:
  - port: 9092
  selector:
    app: kafka
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.4.0
        env:
        - name: KAFKA_BROKER_ID
          value: "1"
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "zookeeper-service:2181" # Birazdan ekleyeceƒüiz
        - name: KAFKA_ADVERTISED_LISTENERS
          value: "PLAINTEXT://kafka-service:9092"
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: "1"
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-service
spec:
  ports:
  - port: 2181
  selector:
    app: zookeeper
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
      - name: zookeeper
        image: confluentinc/cp-zookeeper:7.4.0
        env:
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"

---------------------------------------------------
FILE: ./Makefile
---------------------------------------------------
# Project Variables
IMAGE_NAME := mahmut/analytics-engine:v1
K8S_FILE := k8s/apps.yaml
PORT := 8501
APP_LABEL := app=dashboard-engine
SERVICE_NAME := svc/dashboard-service

.PHONY: all setup check-network clean build deploy wait tunnel logs logs-streamer logs-ai logs-eval help

# Default target
all: help

# --- MAIN WORKFLOW ---

setup: check-network clean build deploy wait
	@echo "---------------------------------------------------"
	@echo "SYSTEM READY. HYBRID AI ENGINE DEPLOYED."
	@echo "---------------------------------------------------"
	@echo "Run 'make tunnel' to access the dashboard."

# --- INDIVIDUAL STEPS ---

# 0. Network Check (Self-Healing)
check-network:
	@echo "Checking Kubernetes cluster connectivity..."
	@kubectl get nodes > /dev/null 2>&1 || \
	(echo "Minikube is stopped or IP changed. Restarting Minikube..." && minikube start)

# 1. Clean Resources
clean:
	@echo "Cleaning up legacy resources and processes..."
	-kubectl delete -f $(K8S_FILE) --ignore-not-found
	-sudo fuser -k $(PORT)/tcp > /dev/null 2>&1
	-pkill -f "kubectl port-forward" > /dev/null 2>&1

# 2. Build Docker Image
# Uses --no-cache to ensure code updates are reflected.
# Uses eval $(minikube docker-env) to build directly inside Minikube.
build:
	@echo "Building Docker image inside Minikube (no-cache)..."
	@eval $$(minikube docker-env) && docker build --no-cache -t $(IMAGE_NAME) .

# 3. Deploy to Kubernetes
deploy:
	@echo "Deploying to Kubernetes..."
	kubectl apply -f $(K8S_FILE)

# 4. Wait for Readiness
wait:
	@echo "Waiting for dashboard pod to be ready (timeout: 120s)..."
	kubectl wait --for=condition=ready pod -l $(APP_LABEL) --timeout=120s

# 5. Port Forwarding
tunnel:
	@echo "Opening tunnel to localhost:$(PORT)..."
	@echo "Access here: http://localhost:$(PORT)"
	kubectl port-forward $(SERVICE_NAME) $(PORT):$(PORT)

# --- UTILITIES (AI MICROSERVICES) ---

logs:
	@echo "Showing Dashboard logs..."
	kubectl logs -l $(APP_LABEL) -f

logs-streamer:
	@echo "Showing Streamer logs..."
	kubectl logs -l app=streamer-engine -f

logs-ai:
	@echo "Showing AI Predictor logs..."
	kubectl logs -l app=predictor-engine -f

logs-eval:
	@echo "Showing AI Evaluator logs..."
	kubectl logs -l app=evaluator-engine -f

help:
	@echo "Available commands:"
	@echo "  make setup         - Fix network, build, clean, deploy and wait."
	@echo "  make tunnel        - Start port forwarding to access the site."
	@echo "  make clean         - Remove all resources."
	@echo "  make logs          - View Dashboard logs."
	@echo "  make logs-streamer - View Data Streamer logs."
	@echo "  make logs-ai       - View AI Predictor logs."
	@echo "  make logs-eval     - View AI Evaluator logs."
---------------------------------------------------
FILE: ./setup.sh
---------------------------------------------------
#!/bin/bash
echo "üßπ Cleaning up zombie processes and ports..."
# 8501 portunu kullanan her seyi (kubectl, streamlit, zombie) temizle
sudo fuser -k 8501/tcp || true
pkill -f "kubectl port-forward" || true

echo "üì¶ Building Docker images inside Minikube..."
eval $(minikube docker-env)
docker build -t mahmut/analytics-engine:v1 ./analytics-engine

echo "üöÄ Deploying to Kubernetes..."
kubectl apply -f k8s/apps.yaml

echo "‚è≥ Waiting for Dashboard to be Ready..."
kubectl wait --for=condition=ready pod -l app=dashboard-engine --timeout=60s

echo "üåâ Opening the Unbreakable Tunnel..."
kubectl port-forward svc/dashboard-service 8501:8501 &

sleep 3
echo "‚ú® SYSTEM READY! Open: http://localhost:8501"

---------------------------------------------------
FILE: ./update_ui.sh
---------------------------------------------------
#!/bin/bash
set -e

# Update dashboard.py with English-only code and no emojis
cat << 'EOF_PY' > analytics-engine/dashboard.py
import os
import time
import json
import pandas as pd
import numpy as np
import streamlit as st
import plotly.graph_objects as go
from kafka import KafkaConsumer
from datetime import datetime
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

# --- Page Config ---
st.set_page_config(page_title="QuantStream AI", layout="wide")
st.title("BTC/USDT AI-POWERED ANALYTICS")

# --- Custom CSS for Report UI ---
st.markdown("""
    <style>
    .report-box {
        background-color: #1E1E1E;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #00ff88;
        margin-top: 20px;
    }
    .report-title {
        color: #00ff88;
        font-size: 20px;
        font-weight: bold;
        margin-bottom: 10px;
    }
    </style>
""", unsafe_allow_html=True)

# --- Kafka Consumer ---
@st.cache_resource
def get_kafka_consumer():
    KAFKA_URL = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka-service:9092')
    try:
        return KafkaConsumer(
            'trade-events',
            bootstrap_servers=[KAFKA_URL],
            auto_offset_reset='earliest',
            value_deserializer=lambda x: json.loads(x.decode('utf-8')),
            consumer_timeout_ms=1000,
            group_id="ai-dashboard-v2"
        )
    except Exception as e:
        st.error(f"Kafka Connection Error: {e}")
        return None

# --- Indicator Calculations ---
def calculate_indicators(df):
    if len(df) < 20: return df
    
    # Bollinger Bands
    df['SMA_20'] = df['price'].rolling(window=20).mean()
    df['StdDev'] = df['price'].rolling(window=20).std()
    df['Upper_Band'] = df['SMA_20'] + (df['StdDev'] * 2)
    df['Lower_Band'] = df['SMA_20'] - (df['StdDev'] * 2)
    
    # RSI
    delta = df['price'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))
    
    # VWAP
    df['VWAP'] = (df['price'] * df['quantity']).cumsum() / df['quantity'].cumsum()
    
    return df

def run_anomaly_detection(df):
    if len(df) < 50: return df
    model = IsolationForest(contamination=0.05, random_state=42)
    data_for_ai = df[['price', 'quantity']].fillna(0)
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data_for_ai)
    df['anomaly'] = model.fit_predict(scaled_data)
    return df

# --- Market Analysis Engine ---
def generate_market_report(df):
    if len(df) < 50: return "Insufficient data for market analysis."
    
    last_price = df['price'].iloc[-1]
    first_price = df['price'].iloc[0]
    price_change = ((last_price - first_price) / first_price) * 100
    
    rsi = df['RSI'].iloc[-1]
    volatility = df['StdDev'].iloc[-1]
    anomalies = df[df['anomaly'] == -1].shape[0] if 'anomaly' in df.columns else 0
    
    trend = "SIDEWAYS"
    if price_change > 0.05: trend = "BULLISH"
    elif price_change < -0.05: trend = "BEARISH"
    
    rsi_comment = "Neutral momentum."
    if rsi > 70: rsi_comment = "Market is OVERBOUGHT. Potential pullback expected."
    elif rsi < 30: rsi_comment = "Market is OVERSOLD. Potential rebound expected."
    
    vol_comment = "Market volatility is stable."
    if volatility > df['price'].mean() * 0.001:
        vol_comment = "High volatility detected. Exercise caution."
        
    report = f"""
    <div class='report-box'>
        <div class='report-title'>Market Intelligence Report (5-Min Summary)</div>
        <ul>
            <li><b>Trend Analysis:</b> The market trend is <b>{trend}</b> with a {price_change:.3f}% price change.</li>
            <li><b>Momentum (RSI):</b> RSI is at {rsi:.1f}. {rsi_comment}</li>
            <li><b>Volatility and Risk:</b> {vol_comment} (StdDev: {volatility:.2f}).</li>
            <li><b>AI Anomaly Scan:</b> Detected <b>{anomalies}</b> anomalous trade patterns in current window.</li>
            <li><b>VWAP Signal:</b> Price is {"ABOVE" if last_price > df['VWAP'].iloc[-1] else "BELOW"} the volume-weighted average price.</li>
        </ul>
        <small><i>Report Generated at: {datetime.now().strftime('%H:%M:%S')}</i></small>
    </div>
    """
    return report

# --- State Management ---
if 'buffer' not in st.session_state:
    st.session_state.buffer = []
if 'last_report_time' not in st.session_state:
    st.session_state.last_report_time = 0
if 'latest_report' not in st.session_state:
    st.session_state.latest_report = "Waiting for data cycle to complete (5 minutes)..."

# --- Real-Time Dashboard Fragment ---
@st.fragment(run_every=1)
def analytics_dashboard():
    consumer = get_kafka_consumer()
    if not consumer: return

    msg_pack = consumer.poll(timeout_ms=500)
    for tp, messages in msg_pack.items():
        for msg in messages:
            trade = msg.value
            st.session_state.buffer.append({
                "time": datetime.now(),
                "price": float(trade.get('p', 0)),
                "quantity": float(trade.get('q', 0))
            })

    if len(st.session_state.buffer) > 500:
        st.session_state.buffer = st.session_state.buffer[-500:]

    if len(st.session_state.buffer) > 20:
        df = pd.DataFrame(st.session_state.buffer)
        df = calculate_indicators(df)
        df = run_anomaly_detection(df)
        
        # 5-Minute Report Update Trigger
        current_time = time.time()
        if current_time - st.session_state.last_report_time > 300:
            st.session_state.latest_report = generate_market_report(df)
            st.session_state.last_report_time = current_time

        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Price and Bollinger Bands")
            fig1 = go.Figure()
            fig1.add_trace(go.Scatter(x=df['time'], y=df['price'], name='Price', line=dict(color='#00ff88')))
            fig1.add_trace(go.Scatter(x=df['time'], y=df['Upper_Band'], name='Upper BB', line=dict(color='gray', dash='dash')))
            fig1.add_trace(go.Scatter(x=df['time'], y=df['Lower_Band'], name='Lower BB', line=dict(color='gray', dash='dash'), fill='tonexty'))
            fig1.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0))
            st.plotly_chart(fig1, use_container_width=True)

        with col2:
            st.subheader("RSI Momentum")
            fig2 = go.Figure(go.Scatter(x=df['time'], y=df['RSI'], name='RSI', line=dict(color='#00ccff')))
            fig2.add_hline(y=70, line_dash="dot", line_color="red")
            fig2.add_hline(y=30, line_dash="dot", line_color="green")
            fig2.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0), yaxis_range=[0,100])
            st.plotly_chart(fig2, use_container_width=True)

        col3, col4 = st.columns(2)

        with col3:
            st.subheader("VWAP Trend")
            fig3 = go.Figure()
            fig3.add_trace(go.Scatter(x=df['time'], y=df['price'], name='Price', line=dict(color='#444')))
            fig3.add_trace(go.Scatter(x=df['time'], y=df['VWAP'], name='VWAP', line=dict(color='#ffa500', width=2)))
            fig3.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0))
            st.plotly_chart(fig3, use_container_width=True)

        with col4:
            st.subheader("AI Anomaly Detection")
            fig4 = go.Figure()
            fig4.add_trace(go.Scatter(x=df['time'], y=df['price'], name='Normal', line=dict(color='#333')))
            if 'anomaly' in df.columns:
                anomalies = df[df['anomaly'] == -1]
                if not anomalies.empty:
                    fig4.add_trace(go.Scatter(x=anomalies['time'], y=anomalies['price'], mode='markers', name='Anomaly', marker=dict(color='red', size=8, symbol='x')))
            fig4.update_layout(template="plotly_dark", height=300, margin=dict(l=0,r=0,t=30,b=0))
            st.plotly_chart(fig4, use_container_width=True)

        st.markdown(st.session_state.latest_report, unsafe_allow_html=True)
    else:
        st.info("Gathering data for initial analysis...")

analytics_dashboard()
EOF_PY

eval $(minikube docker-env)
docker build --no-cache -t mahmut/analytics-engine:v1 ./analytics-engine
kubectl delete pod -l app=dashboard-engine
kubectl wait --for=condition=ready pod -l app=dashboard-engine --timeout=120s
